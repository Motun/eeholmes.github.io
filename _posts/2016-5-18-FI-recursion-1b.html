---
title: "Test big 2"
output:
  html_fragment: 
    self-contained: false
  github_document: default
  eehutils::jekyll_md_document: default
  md_document: default
  pdf_document: default
date: '2016-05-18'
tags:
- Fisher Information
- MARSS
permalink: /posts/2016/05/test-big-2/
---



<script type="text/x-mathjax-config">
  TeX: { equationNumbers: {autoNumber: "AMS"} },
  tex2jax: {inlineMath: [['$$','$$'], ['\\(','\\)']]}
</script>
<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' type='text/javascript'>
</script>

<p>Background on Fisher Information is in <a href="http://parsimoniouspursuits.blogspot.com/2016/05/notes-on-computing-fisher-information.html">Part I</a>.</p>
<div id="computing-fisher-information" class="section level1">
<h1>Computing Fisher Information</h1>
<p>So how do we compute <span class="math inline">\(I(\hat{\theta})\)</span> or <span class="math inline">\(\mathcal{I}(\hat{\theta},y)\)</span> (in Part I)? In particular, can we use the analytical derivatives of the full log-likelihood that are part of the EM algorithm? Many researchers have worked on this idea. My notes here were influenced by <a href="http://statisticalrecipes.blogspot.com/2012/03/em-algorithm-and-confidence-intervals.html">EM Algorithm: Confidence Intervals</a> which is on the same topic. This blog post is mainly a discussion of the result by Louis (1982) on calculation of the Fisher Information matrix from the ‘score’ function that one takes the derivative of in the M-step of the EM algorithm.</p>
The ‘score’ function used in the EM algorithm for a MARSS model is
<span class="math display">\[\begin{equation} 
Q(\theta | \theta_j) = E_{X|y,\theta_j } [\log f_{XY}(X,y|\theta) ]
\end{equation}\]</span>
<p>It is the expected value taken over the hidden random variable <span class="math inline">\(X\)</span> of the full data log-likelihood at <span class="math inline">\(Y=y\)</span> (3); full means it is a function of all the random variables in the model, which includes the hidden or latent variables. <span class="math inline">\(x, y\)</span> is the full ‘data’, the left side of the <span class="math inline">\(x\)</span> state equation and the <span class="math inline">\(y\)</span> observation equation. We take the expectation of this full data likelihood conditioned on the observed data <span class="math inline">\(y\)</span> and <span class="math inline">\(\theta_j\)</span> which is the value of <span class="math inline">\(\theta\)</span> at the j-th iteration of the EM algorithm. Although <span class="math inline">\(Q(\theta | \theta_j)\)</span> looks a bit hairy, actually the full-data likelihood may be very easy to write down and considerably easier than the data likelihood <span class="math inline">\(f(y|\theta)\)</span>. The hard part is often the expectation step, however for MARSS models the Kalman filter-smoother algorithm computes the expectations involving <span class="math inline">\(X\)</span> and Holmes (2010) shows how to compute the expectations involving <span class="math inline">\(Y\)</span>, which comes up when there are missing values in the dataset (missing time steps, say).</p>
In the M-step of the EM algorithm, we take the derivative of <span class="math inline">\(Q(\theta | \theta_j)\)</span> with respect to <span class="math inline">\(\theta\)</span> and solve for the <span class="math inline">\(\theta\)</span> where
<span class="math display">\[\begin{equation}
\frac{\partial Q(\theta | \theta_j ) }{\partial \theta} = 0.
\end{equation}\]</span>
It would be nice if one could use the following to compute the observed Fisher Information
<span class="math display">\[\begin{equation}
-\left.\frac{\partial^2 Q(\theta | \hat{\theta}) }{\partial \theta^2 } \right|_{\theta = \hat{\theta} }
\end{equation}\]</span>
<p><span class="math inline">\(Q(\theta | \hat{\theta})\)</span> is our score function at the end of the EM algorithm, when <span class="math inline">\(\theta = \hat{\theta}\)</span>. <span class="math inline">\(Q\)</span> is a function of <span class="math inline">\(\theta\)</span>, the model parameters, and will have terms like <span class="math inline">\(E(X|Y=y, \hat{\theta})\)</span>, the expected value of <span class="math inline">\(X\)</span> conditioned on <span class="math inline">\(Y=y\)</span> and the MLE. Those are the expectations coming out of the Kalman filter-smoother. We take the second derivative of <span class="math inline">\(Q\)</span> with respect to <span class="math inline">\(\theta\)</span>. That is straight-forward for the MARSS equations. You take the first derivative of <span class="math inline">\(Q\)</span> with respect to <span class="math inline">\(\theta\)</span>, which you already have from the update or M-step equations, and take the derivative of that with respect to <span class="math inline">\(\theta\)</span>.</p>
Conceptually, this
<span class="math display">\[\begin{equation}
-\left.\frac{\partial^2 Q(\theta|\hat{\theta})}{\partial \theta^2} \right|_{\theta = \hat{\theta}} = \left.\frac{\partial^2 E_{X|y,\hat{\theta}}[\log f(X,y|\theta)]}{\partial \theta^2}\right|_{\theta=\hat{\theta}}
\end{equation}\]</span>
looks a bit like the observed Fisher Information:
<span class="math display">\[\begin{equation}\label{obsFI}
\mathcal{I}(\hat{\theta},y) = - \left.\frac{\partial^2\log f(y|\theta)}{\partial \theta^2} \right|_{\theta=\hat{\theta}}
\end{equation}\]</span>
<p>except that instead of the data likelihood <span class="math inline">\(f(y|\theta)\)</span>, we use the expected likelihood <span class="math inline">\(E_{X|y,\hat{\theta} } [\log f_{XY}(X,y|\theta) ]\)</span>. The expected likelihood is the full likelihood with the <span class="math inline">\(X\)</span> and <span class="math inline">\(XX^\top\)</span> random variables replaced by their expected values assuming <span class="math inline">\(\theta = \hat{\theta}\)</span> and <span class="math inline">\(Y=y\)</span>. The problem is that <span class="math inline">\(E_{X|y,\theta } [\log f(X,y|\theta) ]\)</span> is a function of <span class="math inline">\(\theta\)</span> and by fixing it at <span class="math inline">\(\hat{\theta}\)</span> we are not accounting for the uncertainty in that expectation. What we need is something like</p>
<p>Information with <span class="math inline">\(X\)</span> fixed at expected value - Information on expected value of <span class="math inline">\(X\)</span></p>
<p>We account for the fact that we have over-estimated the information from the data by treating the hidden random variable as fixed. The same issue arises when we compute confidence intervals using the estimate of the variance without accounting for the fact that this is an estimate and thus has uncertainty. Louis (1982) and Oakes (1999) are concerned with how to do this correction or adjustment.</p>
<div id="louis-1982-approach" class="section level2">
<h2>Louis 1982 approach</h2>
<p>The following is equations 3.1, 3.2 and 3.3 in Louis (1982) translated to the MARSS case. In the MARSS model, we have two random variables, <span class="math inline">\(X(t)\)</span> and <span class="math inline">\(Y(t)\)</span>. The joint distribution of <span class="math inline">\(\{X(t), Y(t) \}\)</span> conditioned on <span class="math inline">\(X(t-1)\)</span> is multivariate normal. Our full data set includes all time steps, <span class="math inline">\(\{X, Y \}\)</span>.</p>
Let’s call the full state at time t <span class="math inline">\(\{x ,y\}\)</span>, the value of the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> at all times t. The full state can be an unconditional random variable, <span class="math inline">\(\{X,Y\}\)</span> or a conditional random variable <span class="math inline">\(\{X,y\}\)</span> (conditioned on <span class="math inline">\(Y=y\)</span>. Page 227 near top of Louis 1982 becomes
<span class="math display">\[\begin{equation}\label{lambdaz} 
\lambda(x,y,\theta) = \log\{ f_{XY}(x,y|\theta) \} 
\end{equation}\]</span>
<span class="math display">\[\begin{equation}\label{lambday}
\lambda^*(y,\theta) = \log\{ f_Y(y|\theta) \} = \log \int_X f_{XY}(x,y|\theta)dx 
\end{equation}\]</span>
<p><span class="math inline">\(f(.|\theta)\)</span> is the probability distribution of the random variable conditioned on <span class="math inline">\(\theta\)</span>. <span class="math inline">\(\lambda\)</span> is the full likelihood; ‘full’ means is includes both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. <span class="math inline">\(\lambda^*\)</span> is the likelihood of <span class="math inline">\(y\)</span> alone. It is defined by the marginal distribution of <span class="math inline">\(y\)</span> [1]; the integral over <span class="math inline">\(X\)</span> on the right side of . For a MARSS model, the data likelihood can be written easily as a function of the Kalman filter recursions (which is why you can write a recursion for the information matrix based on derivatives of <span class="math inline">\(\lambda^*\)</span>; see Part III).</p>
<p>Next equation down. Louis doesn’t say this and his notation is not totally clear, but the expectation right above section 3 (and in his eqn 3.1) is a conditional expectation. This is critical to know to follow his derivation of equation 3.1 in the appendix. <span class="math inline">\(\theta_j\)</span> is his <span class="math inline">\(\theta(0)\)</span>; it is the value of <span class="math inline">\(\theta\)</span> at the last EM iteration.</p>
<span class="math display">\[\begin{equation}\label{expLL}
E_{X|y,\theta_j}[ \lambda( X, y, \theta)] = \int_X \lambda( X, y, \theta) f_{X|Y}(x|Y=y, \theta_j) dx 
\end{equation}\]</span>
<p>My ‘expectation’ notation is a little different than Louis’. The subscript on the <span class="math inline">\(E\)</span> shows what is being integrated (<span class="math inline">\(X\)</span>) and what are the conditionals.</p>
<p>The term <span class="math inline">\(f_{X|Y}(x|Y=y,\theta_j)\)</span> is the probability of <span class="math inline">\(x\)</span> conditioned on <span class="math inline">\(Y=y\)</span> and <span class="math inline">\(\theta=\theta_j\)</span>. The subscript on <span class="math inline">\(f\)</span> indicates that we are using the probability distribution of <span class="math inline">\(x\)</span> conditioned on <span class="math inline">\(Y=y\)</span>. For the EM algorithm, we need to distinguish between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta_j\)</span> because we maximize with respect to <span class="math inline">\(\theta\)</span> not <span class="math inline">\(\theta_j\)</span>. If we just need the expectation at <span class="math inline">\(\theta\)</span>, no maximization step, then we just use <span class="math inline">\(\theta\)</span> in <span class="math inline">\(f(.|\theta)\)</span> and the subscript on E.</p>
<p>Before moving on with the derivation, notice that in , we fix <span class="math inline">\(y\)</span>, the data. We are not treating that as a random variable. We could certainly treat <span class="math inline">\(E_{\theta_j}[ \lambda( \{X, y\}, \theta)]\)</span> as some function <span class="math inline">\(g(y)\)</span> and consider the random variable <span class="math inline">\(g(Y)\)</span>. But Louis (1982) will not go that route. <span class="math inline">\(y\)</span> is fixed. Thus we are talking about the <em>observed</em> Fisher Information rather than the <em>expected</em> Fisher Information. The latter would take an expectation over the possible <span class="math inline">\(y\)</span> generated by our model with parameters at the MLE.</p>
<div id="derivation-of-equation-3.1-in-louis-1982" class="section level3">
<h3>Derivation of equation 3.1 in Louis 1982</h3>
<p>Now we can derive equation 3.1 in Louis (1982). I am going to combine the info in Louis’ section 3.1 and the appendix on the derivation of 3.1. Before proceeding, Louis is using ‘denominator’ format for his matrix derivations; I normally use denominator format but I will follow his convention here. <span class="math inline">\(\theta\)</span> is a column vector of parameters and the likelihood <span class="math inline">\(f(.|\theta)\)</span> is scalar. Under ‘denominator format’, <span class="math inline">\(f^\prime(.|\theta) = df(.|\theta)/d\theta)\)</span> will be a column vector. <span class="math inline">\(f^{\prime\prime}(.|\theta) = d^2f(.|\theta)/d\theta d\theta^\top)\)</span> will be a matrix in Hessian format (the first <span class="math inline">\(d\theta\)</span> goes 1 to <span class="math inline">\(n\)</span> down columns and the second <span class="math inline">\(d\theta\)</span> does 1 to <span class="math inline">\(n\)</span> across rows).</p>
Take the derivative of  with respect to <span class="math inline">\(\theta\)</span> to define <span class="math inline">\(S(z,\theta)\)</span>.
<span class="math display">\[\begin{equation}
S(x,y,\theta)=\lambda^\prime(x,y,\theta)=\frac{d \log\{f_{XY}(x,y|\theta)\} }{d \theta}= 
\frac{df(x,y|\theta)/d\theta}{f(x,y|\theta)} =
\frac{f^\prime(x,y|\theta)}{f(x,y|\theta)}\label{Sz}
\end{equation}\]</span>
<p>Take the derivative of the far right side of  with respect to <span class="math inline">\(\theta\)</span> to define <span class="math inline">\(S^*(y,\theta)\)</span>. For the last step (far right), I used <span class="math inline">\(f_Y(y|\theta) = \int_X f_{XY}(x,y|\theta)dx\)</span>, the definition of the marginal distribution [1], to change the denominator.</p>
<span class="math display">\[\begin{equation}\label{Sy}
S^*(y,\theta)=\lambda^{*\prime}(y,\theta)=\frac{ d \log \int_X f_{XY}(x,y|\theta)dx  }{d \theta} = 
\frac{ \int_X f_{XY}^\prime(x,y|\theta) dx }{ \int_X f_{XY}(x,y|\theta)dx } =
\frac{ \int_X f_{XY}^\prime(x,y|\theta) dx }{ f_Y(y|\theta) } 
\end{equation}\]</span>
Now multiply the integrand in the numerator by <span class="math inline">\(f_{XY}(x,y|\theta)/f_{XY}(x,y|\theta)\)</span>. The last step (far right) uses .
<span class="math display">\[\begin{equation}\label{intfprime}
\begin{gathered}
\int_X f_{XY}^\prime(x,y|\theta) dx   =
\int_X \frac{f_{XY}^\prime(x,y|\theta)f_{XY}(x,y|\theta)}{f_{XY}(x,y|\theta)} dx = \\
\int_X \frac{f_{XY}^\prime(x,y|\theta)}{f_{XY}(x,y|\theta)}f_{XY}(x,y|\theta) dx =
\int_X S(x,y,\theta) f_{XY}(x,y|\theta) dx 
\end{gathered}
\end{equation}\]</span>
We combine  and :
<span class="math display">\[\begin{equation}\label{Sstar}
S^*(y,\theta)=
\frac{ \int_X f_{XY}^\prime(x,y|\theta) dx }{ f_Y(y|\theta) } =
\int_X S(x,y,\theta)  \frac{ f_{XY}(x,y|\theta) }{ f_Y(y|\theta) } dx =
\int_X S(x,y,\theta) f_{X|Y}(x|Y=y,\theta) dx
\end{equation}\]</span>
<p>The second to last step used the fact that <span class="math inline">\(f_Y(y|\theta)\)</span> does not involve <span class="math inline">\(x\)</span> thus we can bring it into the integral. This gives us <span class="math inline">\(f_{XY}(x,y|\theta)/f_Y(y|\theta)\)</span>. This is the probability of <span class="math inline">\(x\)</span> conditioned on <span class="math inline">\(Y=y\)</span> [2].</p>
The last step in the derivation of equation 3.1 is to recognize that the far right side of  is the conditional expectation in 3.1. Louis does not actually write out the expectation in 3.1 and the notation is rather vague. But the expectation in equation 3.1 is the conditional expectation on the far right side of .
<span class="math display">\[\begin{equation}\label{Louise3p1}
S^*(y,\theta)=\int_X S(x,y,\theta) f_{X|Y}(x|Y=y,\theta) dx=E_{X|y,\theta} [ S(X,y,\theta) ]
\end{equation}\]</span>
<p>using my notation for a conditional expectation which slightly different than Louis’. At the MLE, <span class="math inline">\(S^*(y,\hat{\theta})=0\)</span> since that is how the MLE is defined (it’s where the derivative of the data likelihood is zero).</p>
</div>
<div id="derivation-of-equation-3.2-in-louis-1982" class="section level3">
<h3>Derivation of equation 3.2 in Louis 1982</h3>
The meat of Louis 1982 is equation 3.2. The observed Fisher Information matrix  is
<span class="math display">\[\begin{equation}\label{obsFI32}
 \mathcal{I}(\theta,y) = B^*(y,\theta) = -S^\prime(x,y,\theta) = - \lambda^{*\prime\prime}(y,\theta) = -\frac{\partial^2\log f_Y(y|\theta)}{\partial \theta \partial \theta^\top} 
\end{equation}\]</span>
<p>The first 3 terms on the left are just show that all are notation that refers to the observed Fisher Information. The 4th term is one of the ways we can compute the observed Fisher Information at <span class="math inline">\(\theta\)</span> and the far right term shows that derivative explicitly.</p>
We start by taking the second derivative of  with respect to <span class="math inline">\(\theta\)</span> to define <span class="math inline">\(B(x,y,\theta)\)</span>. We use <span class="math inline">\(S^\prime(z,\theta)\)</span> as written in .
<span class="math display">\[\begin{equation}\label{B1}
\mathcal{I}(\theta,x,y) = B(x,y,\theta)=-\lambda^{\prime\prime}(x,y,\theta) = -S^\prime(x,y,\theta) = 
-\frac{d[f_{XY}^\prime(x,y|\theta)/f_{XY}(x,y|\theta)]}{d \theta^\top}
\end{equation}\]</span>
<p>The transpose of <span class="math inline">\(d\theta\)</span> is because we are taking the second derivative <span class="math inline">\(d^2 l/d\theta d\theta^\top\)</span> (the Hessian of the log-likelihood); <span class="math inline">\(d\theta d\theta\)</span> wouldn’t make sense as that that would be a column vector times a column vector.</p>
To do the derivative on the far right side of , we first need to recognize the form of the equation. <span class="math inline">\(f_{XY}^\prime(x,y|\theta)\)</span> is a column vector and <span class="math inline">\(f(x,y|\theta)\)</span> is a scalar, thus the thing we are taking the derivative of has the form <span class="math inline">\(\overrightarrow{h}(\theta)/g(\theta)\)</span>; the arrow over <span class="math inline">\(h\)</span> is indicating that it is a (column) vector while <span class="math inline">\(g()\)</span> is a scalar. Using the chain rule for vector derivatives, we have
<span class="math display">\[\begin{equation}
\frac{ d (\overrightarrow{h}(\theta)/g(\theta))}{d \theta^\top} = \frac{d\overrightarrow{h}(\theta)}{d \theta^\top}\frac{1}{g(\theta)} - \frac{\overrightarrow{h}(\theta)}{ g(\theta)^2 }\frac{ g(\theta) }{ d \theta^\top }
\end{equation}\]</span>
<p>Thus (notice I’m writing the equation for the negative of <span class="math inline">\(B(x,y,\theta)\)</span> ),</p>
<span class="math display">\[\begin{align}\label{B2}
-B(x,y,\theta) =&amp; 
\frac{d(f_{XY}^\prime(x,y|\theta)/f_{XY}(x,y|\theta))}{d \theta^\top} \\
=&amp; \frac{f_{XY}^{\prime\prime}(x,y|\theta)}{f_{XY}(x,y|\theta)}
-\frac{f_{XY}^\prime(x,y|\theta)f^\prime(z|\theta)^\top}{ f_{XY}(x,y|\theta)^2 } \\
=&amp; \frac{f_{XY}^{\prime\prime}(x,y|\theta)}{f_{XY}(x,y|\theta)} - S(x,y|\theta)S(x,y|\theta)^\top
\end{align}\]</span>
<p>Let’s return to  and take the derivative of <span class="math inline">\(\lambda^{*\prime}(y,\theta)\)</span> with respect to <span class="math inline">\(\theta\)</span> using the form shown in equation . I have replaced the integral in the denominator by <span class="math inline">\(f_Y(y|\theta)\)</span> and used the same chain rule used for .</p>
<span class="math display">\[\begin{align}
\begin{split}
\lambda^{*\prime\prime}(y,\theta)=
d\left( \int_X f_{XY}^\prime(x,y|\theta) dx \middle/ f_Y(y|\theta) \right)/d\theta^\top = \\
\frac{\int_X f_{XY}^{\prime\prime}(x,y|\theta) dx }{f_Y(y|\theta)}-
\frac{\int_X f_{XY}^\prime(x,y|\theta)dx }{f_Y(y|\theta)} \left(\frac{\int_X f_{XY}^\prime(x,y|\theta)dx}{f_Y(y|\theta)}\right) =
\frac{\int_X f_{XY}^{\prime\prime}(x,y|\theta) dx }{f_Y(y|\theta)}-
S^*(y|\theta)S^*(y|\theta)^\top
\end{split}
\end{align}\]</span>
The last substitution uses . Thus,
<span class="math display">\[\begin{equation}\label{B4}
\lambda^{*\prime\prime}(y,\theta)=
\frac{\int_X f_{XY}^{\prime\prime}(x,y|\theta) dx }{f_Y(y|\theta)}-
S^*(y|\theta)S^*(y|\theta)^\top
\end{equation}\]</span>
<p>Let’s look at the integral of the second derivative of <span class="math inline">\(f_{XY}(x,y|\theta)\)</span> in :</p>
<span class="math display">\[\begin{equation}\label{B5}
\begin{gathered}
\left( \int_X f_{XY}^{\prime\prime}(x,y|\theta) dx \middle/ f_Y(y|\theta) \right) = \\
 \int_X \frac{f_{XY}^{\prime\prime}(x,y|\theta) dx}{ f_{XY}(x,y|\theta) }\frac{f_{XY}(x,y|\theta)}{ f_Y(y|\theta)} dx=
 \int_X \frac{f_{XY}^{\prime\prime}(x,y|\theta) dx}{ f_{XY}(x,y|\theta) }f_{X|Y}(x|Y=y,\theta) dx
\end{gathered}
\end{equation}\]</span>
This is the conditional expectation <span class="math inline">\(E_{X|y,\theta} [ f_{XY}^{\prime\prime}(x,y|\theta) dx/f_{XY}(x,y|\theta) ]\)</span> that we see 5 lines above the references in Louis (1982). Using  we can write this in terms of <span class="math inline">\(B(x,y|\theta)\)</span>:
<span class="math display">\[\begin{equation}\label{B6}
\int_X \frac{f_{XY}^{\prime\prime}(z|\theta) dx}{ f_{XY}(x,y|\theta) } = -B(x,y|\theta)+S(x,y|\theta)S(x,y|\theta)^\top
\end{equation}\]</span>
Combining , , and , we can write the equation above the references in Louis:
<span class="math display">\[\begin{equation}\label{B7}
\lambda^{*\prime\prime}(y,\theta)= E_{X|y,\theta} [ - B(X,y|\theta)+S(X,y|\theta)S(X,y|\theta)^\top]-S^*(y|\theta)S^*(y|\theta)^\top
\end{equation}\]</span>
The negative of this is the observed Fisher Information () which gives us equation 3.2 in Louis (1982):
<span class="math display">\[\begin{equation}\label{Louismain}
\mathcal{I}(\theta,y) = E_{X|y,\theta} [ B(X,y|\theta)] - E_{X|y,\theta} [ S(X,y|\theta)S(X,y|\theta)^\top]+S^*(y|\theta)S^*(y|\theta)^\top
\end{equation}\]</span>
</div>
<div id="derivation-of-equation-3.3-in-louis-1982" class="section level3">
<h3>Derivation of equation 3.3 in Louis 1982</h3>
Louis states that “The first term in (3.2) is the conditional expected full data observed information matrix, while the last two produce the expected information for the conditional distribution of X given <span class="math inline">\(X \in R\)</span>.” His X is my <span class="math inline">\(\{X,Y\}\)</span> and <span class="math inline">\(X \in R\)</span> means <span class="math inline">\(Y=y\)</span> in my context. He writes this in simplified form with <span class="math inline">\(X\)</span> replaced by <span class="math inline">\(XY\)</span>:
<span class="math display">\[\begin{equation}
I_Y = I_{XY} - I_{X|Y}
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
\mathcal{I}(\theta,y) = E_{X|y,\theta} [\mathcal{I}(\theta,X,y)] - I_{X|Y}
\end{equation}\]</span>
<p>Let’s see how this is the case.</p>
The full data observed information matrix is
<span class="math display">\[\begin{equation}
\mathcal{I}(\theta,x,y) = -\lambda^{\prime\prime}(x,y|\theta) = B(x,y,\theta)
\end{equation}\]</span>
This is simply the definition that Louis gives to <span class="math inline">\(B(x,y,\theta)\)</span>. We do not know <span class="math inline">\(x\)</span> so we do not know the full data observed Information matrix. But we have the distribution of <span class="math inline">\(x\)</span> conditioned on our data <span class="math inline">\(y\)</span>.
<span class="math display">\[\begin{equation}
E_{X|y,\theta} [ B(X,y|\theta)]
\end{equation}\]</span>
is thus the expected full data observed information matrix conditioned on our observed data <span class="math inline">\(y\)</span>. So this is the first part of his statement.<br /><br /> The second part of his statement takes a bit more effort to work out. First we substitute <span class="math inline">\(S^*(y|\theta)\)</span> with <span class="math inline">\(E_{X|y,\theta} [ S(X,y|\theta) ]\)</span> from . This gives us this:
<span class="math display">\[\begin{equation}\label{ES1}
\begin{gathered}
 E_{X|y,\theta} [ S(X,y|\theta)S(X,y|\theta)^\top ]-S^*(y|\theta)S^*(y|\theta)^\top = \\
E_{X|y,\theta} [ S(X,y|\theta)S(X,y|\theta)^\top ]-E_{X|y,\theta} [ S(X,y|\theta) ]E_{X|y,\theta} [ S(X,y|\theta)^\top ]
\end{gathered}
\end{equation}\]</span>
Using the computational form of the variance, <span class="math inline">\(var(X)=E(XX)-E(X)E(X)\)</span>, we can see that  is the conditional variance of <span class="math inline">\(S(X,y|\theta)\)</span>.
<span class="math display">\[\begin{equation}
var_{X|y,\theta}( S(X,y|\theta) )
\end{equation}\]</span>
<p>But the variance of the first derivative of <span class="math inline">\(f^\prime(X|\theta)\)</span> is the <em>expected</em> Fisher Information of <span class="math inline">\(X\)</span> [4]. In this case, it is the expected Fisher Information of the hidden state <span class="math inline">\(X\)</span>, where we specify that <span class="math inline">\(X\)</span> has the conditional distribution <span class="math inline">\(f_{X|Y} (X | Y=y,\theta)\)</span>. Thus we have the second part of Louis’ statement.</p>
</div>
</div>
<div id="relating-louis-1982-to-the-update-equations-in-the-marss-em-algorithm" class="section level2">
<h2>Relating Louis 1982 to the update equations in the MARSS EM algorithm</h2>
The main result in Louis (1982) () can be written
<span class="math display">\[\begin{equation}\label{Louismain2}
\mathcal{I}(\theta,y) = E_{X|y,\theta} [ B(X,y|\theta)] - var_{X|y,\theta} [ S(X,y|\theta) ]
\end{equation}\]</span>
The M-step of the EM algorithm involves the first derivative of the log-likelihood with respect to <span class="math inline">\(\theta\)</span>, <span class="math inline">\(S(X,y|\theta)\)</span>, since it involves setting this derivative to zero:
<span class="math display">\[\begin{equation} 
Q^\prime(\theta | \theta_j) = d( E_{X|y,\theta_j } [\log f_{XY}(X,y|\theta) ])/d\theta = E_{X|y,\theta_j } [\log f^\prime_{XY}(X,y|\theta) ] = E_{X|y,\theta_j } [ S(X,y|\theta) ]
\end{equation}\]</span>
<p>With the MARSS model, <span class="math inline">\(S(X,y|\theta)\)</span> is analytical and we can also compute <span class="math inline">\(B(X,y|\theta)\)</span>, the second derivative, analytically.</p>
<p>The difficulty arises with this term: <span class="math inline">\(var_{X|y,\theta} [ S(X,y|\theta) ]\)</span>. The <span class="math inline">\(S(X,y|\theta)\)</span> is a summation from <span class="math inline">\(t=1\)</span> to <span class="math inline">\(T\)</span> that involves <span class="math inline">\(X_t\)</span> or <span class="math inline">\(X_t X_{t-1}^top\)</span> for some parameters. When we do the cross-product, we will end up with terms like <span class="math inline">\(E[ X_t X_{t+k}^\top ]\)</span> and <span class="math inline">\(E[ X_t X_t^\top X_{t+k}X_{t+k}^\top ]\)</span>. The latter is not a problem; all the random variables in a MARSS models are multivariate normal and the k-th central moments can be expressed in terms of the first and second moments [5], but that will still leave us with terms like <span class="math inline">\(E[ X_t X_{t+k}^\top ]\)</span>—the smoothed covariance between <span class="math inline">\(X\)</span> at time <span class="math inline">\(t\)</span> and <span class="math inline">\(t+k\)</span> conditioned on all the data (<span class="math inline">\(t=1:T\)</span>).</p>
<p>Computing these is not hard. These are the the n-step apart smoothed covariances. Harvey (1989), page 148, discusses how to use the Kalman filter to get the n-step ahead prediction covariances and a similar approach can be used (presumably) to get the <span class="math inline">\(V(t,t+k)\)</span> smoothed covariances. However this will end up being computationally expensive because we will need all of the <span class="math inline">\(t,t+k\)</span> combinations, i.e., {1,3}, {1,4}, …, {2,3}, {2,4}, …. etc.. That will be a lot: T + T-1 + T-2 + T-3 = <span class="math inline">\(T(T+1)/2\)</span>, smoothed covariances.<br />
Lystig and Hughes (2012) and Duan and Fulop (2011) discuss this issue for in a related application of the approach in Louis (1982). They suggest that you do not need to include covariances with a large time separation because the covariance goes to zero. You just need to include enough time-steps.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
I think the approach of Louis (1982) is not viable for MARSS models. The derivatives <span class="math inline">\(B(x,y|\theta)\)</span> and <span class="math inline">\(S(x,y|\theta)\)</span> are straight-forward (if tedious) to compute analytically following the approach in Holmes (2010). But the computing all the n-step smoothed covariances is going to be very slow and each computation involves many matrix multiplications. However, one could compute <span class="math inline">\(\mathcal{I}(\theta,y)\)</span> via simulation using . It is easy enough to simulate <span class="math inline">\(X\)</span> using the MLEs and then you compute <span class="math inline">\(B(x_b,y|\theta)\)</span> and <span class="math inline">\(S(x_b,y|\theta)\)</span> for each where <span class="math inline">\(x_b\)</span> is the bootstrapped <span class="math inline">\(x\)</span> time series and <span class="math inline">\(y\)</span> is the data. I don’t think it makes sense to do that for MARSS models since there are two recursion approaches for computing the observed and expected Fisher Information using <span class="math inline">\(f(y|\theta)\)</span> and the Kalman filter equations (Harvey 1989, pages 140-142; Cavanaugh and Shumway 1996).
<hr>
</div>
<div id="footnotes" class="section level1">
<h1>Footnotes</h1>
[1] Given a joint probability distribution of <span class="math inline">\(\{X,Y\}\)</span>, the marginal distribution of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\int_X f(X,Y) dx\)</span>. Discussions of the estimators for MARSS models often use the property of the marginal distributions of a multivariate normal without actually stating that this property is being used. The step in the derivation will just say, ‘Thus’ with no indication of what property was just used. <br />Reviewed here: <a href="http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html" class="uri">http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html</a> If you have a joint likelihood of some random variables, and you want the likelihood of a subset of those random variables, then you compute the marginal distribution. i.e. you integrate over the random variables you want to get rid of:
<span class="math display">\[\begin{equation}
L(\theta | y) ] = \int_X L(\theta | X,Y) p(x|Y=y, \theta_j) dx |_{Y=y}.
\end{equation}\]</span>
<p>So we integrate out <span class="math inline">\(X\)</span> from the full likelihood and then set <span class="math inline">\(Y=y\)</span> to get the</p>
The marginal likelihood is a little different. The marginal likelihood is used when you want to get rid of some of the parameters, nuisance parameters. The integral you use is different:
<span class="math display">\[\begin{equation}
L(\theta_1|y) = \int_{\theta_2} p(y|\theta_1,\theta_2) p(\theta_2|\theta_1)d\theta_2 
\end{equation}\]</span>
<p>This presumes that you have <span class="math inline">\(p(\theta_2|\theta_1)\)</span>.</p>
The expected likelihood is different yet again:<br />

<span class="math display">\[\begin{equation}
E_{X,Y|Y=y,\theta_j} [L(\theta | X,Y) ] = \int_X L(\theta | X,Y) p(x|Y=y, \theta_j) dx.
\end{equation}\]</span>
<p>On the surface it looks like the equation for <span class="math inline">\(L(\theta|y)\)</span> but it is different. <span class="math inline">\(\theta_j\)</span> is not <span class="math inline">\(\theta\)</span>. It is the parameter value at which we are computing the expected value of <span class="math inline">\(X\)</span>. Maximizing the <span class="math inline">\(E_{X,Y|Y=y,\theta_j} [L(\theta | X,Y) ]\)</span> will increase the likelihood but will not take you to the MLE—you have to imbed this maximization in the EM algorithm that walks up the likelihood surface.</p>
<p>[2] <span class="math inline">\(P(A|B) = P(A \cup B)/P(B)\)</span></p>
<p>[3] I normally think about <span class="math inline">\(Y\)</span> as being partially observed (missing values) so I also take the expectation over <span class="math inline">\(Y(2)\)</span> conditioned on <span class="math inline">\(Y(1)\)</span>, where (1) means observed and (2) means missing. In Holmes (2010), this is done in order to derive general EM update equations for the missing values case. But my notation is getting hairy, so for this write-up, I’m treating <span class="math inline">\(Y\)</span> as fully observed; so no <span class="math inline">\(Y(2)\)</span> and I’ve dropped the integrals (expectations) over <span class="math inline">\(Y(2)\)</span>.</p>
<p>[4] <a href="http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf">http://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf</a></p>
<p>[5] <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Higher_moments" class="uri">https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Higher_moments</a></p>
</div>
<div id="papers-and-online-references" class="section level1">
<h1>Papers and online references</h1>
<ul>
<li><p>Ng, Krishnan and McLachlan 2004<br /> The EM algorithm. Section 3.5 discusses standard errors approaches<br /> <a href="https://www.econstor.eu/dspace/bitstream/10419/22198/1/24_tk_gm_skn.pdf" class="uri">https://www.econstor.eu/dspace/bitstream/10419/22198/1/24_tk_gm_skn.pdf</a><br /> <a href="http://hdl.handle.net/10419/22198" class="uri">http://hdl.handle.net/10419/22198</a></p></li>
<li><p>Efron and Hinkley 1978<br /> (argues that the observed Fisher Information is better than expected Fisher Information in many/some cases. The same paper argues for the likelihood ratio method for CIs)<br /> Assessing the accuracy of the maximum likelihood estimator: observed versus expected Fisher Information<br /> <a href="https://www.stat.tamu.edu/~suhasini/teaching613/expected_observed_information78.pdf" class="uri">https://www.stat.tamu.edu/~suhasini/teaching613/expected_observed_information78.pdf</a></p></li>
<li><p>Hamilton 1994<br /> <a href="http://web.pdx.edu/~crkl/readings/Hamilton94.pdf" class="uri">http://web.pdx.edu/~crkl/readings/Hamilton94.pdf</a></p></li>
<li><p>Hamilton’s exposition assumes you know the marginal distribution of a multivariate normal. Scroll down to the bottom.<br /> <a href="http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html" class="uri">http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html</a></p></li>
<li><p>Meilijson 1989<br /> Fast improvement to the EM algorithm on its own terms<br /> <a href="http://www.jstor.org/stable/pdf/2345847.pdf" class="uri">http://www.jstor.org/stable/pdf/2345847.pdf</a></p></li>
<li><p>Oakes 1999<br /> Direct calculation of the information matrix via the EM algorithm<br /> <a href="http://www.jstor.org/stable/pdf/2680653.pdf?_=1463187953783" class="uri">http://www.jstor.org/stable/pdf/2680653.pdf?_=1463187953783</a></p></li>
<li><p>Ho, Shumway and Ombao 2006<br /> (this has a brief statement that Oakes 1999 derivatives are hard to compute. It doesn’t say why. It says nothing of Louis 1982.)<br /> Chapter 7, The state-space approach to modeling dynamic processes<br /> Models for Intensive Longitudinal Data<br /> <a href="https://books.google.com/books?hl=en&amp;lr" class="uri">https://books.google.com/books?hl=en&amp;lr</a>=&amp;id=Semo20xZ_M8C</p></li>
<li><p>Louis 1982<br /> (so elegant. alas, MARSS deals with time series data…)<br /> Finding the observed information matrix when using the EM algorithm<br /> <a href="http://www.jstor.org/stable/pdf/2345828.pdf" class="uri">http://www.jstor.org/stable/pdf/2345828.pdf</a><br /> <a href="http://www.markirwin.net/stat220/Refs/louis1982.pdf" class="uri">http://www.markirwin.net/stat220/Refs/louis1982.pdf</a></p></li>
<li><p>Lystig and Hughes 2012 <br /> (helped me better understand why Louis 1982 is hard for MARSS models)<br /> Exact computation of the observed information matrix for hidden Markov models<br /> <a href="http://www.tandfonline.com.offcampus.lib.washington.edu/doi/abs/10.1198/106186002402" class="uri">http://www.tandfonline.com.offcampus.lib.washington.edu/doi/abs/10.1198/106186002402</a></p></li>
<li><p>Duan and Fulop 2011<br /> (also helped me better understand why Louis 1982 is hard for MARSS models)<br /> A stable estimator for the information matrix under EM for dependent data<br /> <a href="http://www.rmi.nus.edu.sg/DuanJC/index_files/files/EM_Variance_March%205%202007.pdf">http://www.rmi.nus.edu.sg/DuanJC/index_files/files/EM_Variance_March%205%202007.pdf</a><br /> <a href="http://link.springer.com/article/10.1007/s11222-009-9149-4" class="uri">http://link.springer.com/article/10.1007/s11222-009-9149-4</a></p></li>
<li><p>Naranjo 2007 (didn’t use)<br /> State-space models with exogenous variables and missing data, PhD U of FL<br /> <a href="http://etd.fcla.edu/UF/UFE0021568/naranjo_a.pdf" class="uri">http://etd.fcla.edu/UF/UFE0021568/naranjo_a.pdf</a></p></li>
<li><p>Dempster, Laird, Rubin 1977<br /> (didn’t really use but looked up more info on the ‘score’ function Q)<br /> Maximum likelihood for incomplete data via the EM algorithm<br /> <a href="http://web.mit.edu/6.435/www/Dempster77.pdf" class="uri">http://web.mit.edu/6.435/www/Dempster77.pdf</a></p></li>
<li><p>van Dyk, Meng and Rubin 1995<br /> (this looks promising)<br /> Maximum likelihood estimation via the ECM algorithm: computing the asymptotic variance<br /> <a href="http://wwwf.imperial.ac.uk/~dvandyk/Research/95-sinica-secm.pdf" class="uri">http://wwwf.imperial.ac.uk/~dvandyk/Research/95-sinica-secm.pdf</a></p></li>
<li><p>Cavanaugh and Shumway 1996<br /> On computing the expected Fisher Information Matrix for state-space model parameters<br /></p></li>
<li><p>Harvey 1989, pages 140-143, Section 3.4.5 Information matrix<br /> Forecasting, structural time series models and the Kalman filter<br /></p></li>
</ul>
</div>

